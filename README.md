# Leveraging LLMs for K-12 Education Assessment: An Open-Ended Question Answering Use Case
Repository of the article "Leveraging LLMs for K-12 Education Assessment: An Open-Ended Question Answering Use Case" submitted to the IEEE URUCON 2024 conference.

The repository includes:
* The prompt used to generate the responses corresponding to the different gradings: excellent, acceptable and incorrect ([prompt_dataset.txt](https://github.com/gcapde/eval_llms_edutech_assessment/blob/main/prompt_dataset.txt)).
* The different prompts tested with several LLMs ([prompts_URUCON.txt](https://github.com/gcapde/eval_llms_edutech_assessment/blob/main/prompts_URUCON.txt) and [inverted_prompts_URUCON.txt](https://github.com/gcapde/eval_llms_edutech_assessment/blob/main/inverted_prompts_URUCON.txt)).
* Configuration file to run the tests with promptfoo ([promptfooconfig_URUCON.yml](https://github.com/gcapde/eval_llms_edutech_assessment/blob/main/promptfooconfig_URUCON.yml)).
* Python code used to analyze the experiments results (JSON files from promptfoo) and generate the graphs included in the paper ([analysis_results.py](https://github.com/gcapde/eval_llms_edutech_assessment/blob/main/analysis_results.py) and [Accuracy.py](https://github.com/gcapde/eval_llms_edutech_assessment/blob/main/Accuracy.ipynb)).

![imagen](https://github.com/gcapde/eval_llms_edutech_assessment/assets/29692811/c7f3efb1-bb35-425e-bcc7-519533407fcb)

